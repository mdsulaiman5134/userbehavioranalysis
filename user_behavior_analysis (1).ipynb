{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c2210af5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load the CSV from the DBFS path\n",
    "df = spark.read.option(\"header\", True).option(\"inferSchema\", True).csv(\"/FileStore/tables/user_behavior.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5314d6c5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Show schema\n",
    "df.printSchema()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "19d42616",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Show sample data\n",
    "df.show(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2ff6a64e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Count rows\n",
    "df.count()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a9544bec",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Unique users\n",
    "df.select(\"user_id\").distinct().count()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3721b8e2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Most common events\n",
    "df.groupBy(\"event_type\").count().orderBy(\"count\", ascending=False).show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9bcdb4d4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Most visited screens\n",
    "df.groupBy(\"screen_name\").count().orderBy(\"count\", ascending=False).show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "20d9b9f1",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql.functions import col, to_timestamp\n",
    "\n",
    "# Convert 'event_time' to timestamp format\n",
    "df = df.withColumn(\"event_time\", to_timestamp(col(\"event_time\")))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6b17a8af",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Sort by user_id, session_id, and event_time\n",
    "df = df.orderBy(\"user_id\", \"session_id\", \"event_time\")\n",
    "\n",
    "df.show(30)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f26b4964",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql.functions import max, min, col\n",
    "\n",
    "# Group by user_id and session_id, then calculate session start and end\n",
    "session_duration = df.groupBy(\"user_id\", \"session_id\") \\\n",
    "    .agg(\n",
    "        min(\"event_time\").alias(\"session_start\"),\n",
    "        max(\"event_time\").alias(\"session_end\")\n",
    "    ) \\\n",
    "    .withColumn(\"duration_minutes\", (col(\"session_end\").cast(\"long\") - col(\"session_start\").cast(\"long\")) / 60)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "278f2d31",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Show the longest sessions\n",
    "session_duration.orderBy(\"duration_minutes\", ascending=False).show(15)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "11228e7e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Most visited screens\n",
    "df.groupBy(\"screen_name\") \\\n",
    "  .count() \\\n",
    "  .orderBy(\"count\", ascending=False) \\\n",
    "  .show(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7b11e4b7",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql.window import Window\n",
    "from pyspark.sql.functions import row_number\n",
    "\n",
    "# Create a window partitioned by user and session, ordered by time DESC\n",
    "w = Window.partitionBy(\"user_id\", \"session_id\").orderBy(col(\"event_time\").desc())\n",
    "\n",
    "# Pick the last event in each session\n",
    "last_events = df.withColumn(\"rank\", row_number().over(w)) \\\n",
    "    .filter(col(\"rank\") == 1) \\\n",
    "    .groupBy(\"screen_name\") \\\n",
    "    .count() \\\n",
    "    .orderBy(\"count\", ascending=False)\n",
    "\n",
    "last_events.show(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "248bd6fb",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql import functions as F\n",
    "from pyspark.sql.window import Window\n",
    "\n",
    "# Create a row number per user session ordered by event time\n",
    "w = Window.partitionBy(\"user_id\", \"session_id\").orderBy(\"event_time\")\n",
    "df_with_order = df.withColumn(\"step\", F.row_number().over(w))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "905490ee",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Self join to get next screen after each event\n",
    "transitions = df_with_order.alias(\"a\").join(\n",
    "    df_with_order.alias(\"b\"),\n",
    "    on=[\n",
    "        F.col(\"a.user_id\") == F.col(\"b.user_id\"),\n",
    "        F.col(\"a.session_id\") == F.col(\"b.session_id\"),\n",
    "        F.col(\"a.step\") + 1 == F.col(\"b.step\")\n",
    "    ],\n",
    "    how=\"inner\"\n",
    ").select(\n",
    "    F.col(\"a.screen_name\").alias(\"from_screen\"),\n",
    "    F.col(\"b.screen_name\").alias(\"to_screen\")\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "76a57785",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Count screen transitions\n",
    "transition_counts = transitions.groupBy(\"from_screen\", \"to_screen\").count().orderBy(\"count\", ascending=False)\n",
    "transition_counts.show(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d552da15",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Aggregate user stats\n",
    "user_stats = session_duration.groupBy(\"user_id\").agg(\n",
    "    F.count(\"session_id\").alias(\"num_sessions\"),\n",
    "    F.avg(\"duration_minutes\").alias(\"avg_duration\")\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b4fdad29",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Optional: Join with total events per user\n",
    "event_counts = df.groupBy(\"user_id\").count().withColumnRenamed(\"count\", \"total_events\")\n",
    "user_features = user_stats.join(event_counts, on=\"user_id\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b454fc29",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.ml.feature import VectorAssembler\n",
    "from pyspark.ml.clustering import KMeans\n",
    "\n",
    "# Feature vector\n",
    "assembler = VectorAssembler(inputCols=[\"num_sessions\", \"avg_duration\", \"total_events\"], outputCol=\"features\")\n",
    "dataset = assembler.transform(user_features)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7316a2bc",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Apply KMeans\n",
    "kmeans = KMeans(k=3, seed=1)\n",
    "model = kmeans.fit(dataset)\n",
    "clusters = model.transform(dataset)\n",
    "\n",
    "clusters.select(\"user_id\", \"prediction\").show(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3286c897",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# Convert to Pandas for plotting\n",
    "pandas_df = clusters.select(\"num_sessions\", \"avg_duration\", \"total_events\", \"prediction\").toPandas()\n",
    "\n",
    "# Plot clusters\n",
    "plt.scatter(pandas_df['avg_duration'], pandas_df['total_events'], c=pandas_df['prediction'], cmap='viridis')\n",
    "plt.xlabel(\"Avg Session Duration\")\n",
    "plt.ylabel(\"Total Events\")\n",
    "plt.title(\"User Segments\")\n",
    "plt.grid(True)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "98613818",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Drop the features column before exporting\n",
    "clusters_cleaned = clusters.drop(\"features\")\n",
    "\n",
    "# Convert to Pandas safely\n",
    "pandas_df = clusters_cleaned.toPandas()\n",
    "\n",
    "# Export to CSV\n",
    "pandas_df.to_csv(\"/tmp/user_behavior.csv\", index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cdb1f1e7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Move the CSV to a web-accessible location\n",
    "dbutils.fs.mv(\"file:/tmp/user_behavior.csv\", \"dbfs:/FileStore/user_behavior.csv\")"
   ]
  }
 ],
 "metadata": {},
 "nbformat": 4,
 "nbformat_minor": 5
}
